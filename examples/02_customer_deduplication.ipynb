{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning CRM Data: Finding and Grouping Duplicate Customer Records\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "Every CRM database accumulates duplicates over time. The same customer appears multiple times with slight variations:\n",
    "\n",
    "| Record # | Name | Source |\n",
    "|----------|------|--------|\n",
    "| 1001 | John Smith | Web form |\n",
    "| 1042 | Jon Smith | Sales import |\n",
    "| 1089 | JOHN SMITH | Call center |\n",
    "| 1156 | John A. Smith | Partner API |\n",
    "\n",
    "These are all the same person, but your database treats them as four different customers. This causes:\n",
    "\n",
    "- **Fragmented customer history**: Purchase records split across accounts\n",
    "- **Wasted marketing spend**: Same person receives multiple emails\n",
    "- **Poor customer experience**: \"You're not in our system\" when they clearly are\n",
    "- **Inaccurate analytics**: Customer count inflated, lifetime value understated\n",
    "\n",
    "Manual deduplication doesn't scale. A database with 100,000 customers has nearly 5 billion possible pairs to compare.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this tutorial, we'll build an automated deduplication pipeline:\n",
    "\n",
    "1. **Understanding name variations**: Why \"Jon\" matches \"John\" but not \"Jane\"\n",
    "2. **Choosing the right algorithm**: Jaro-Winkler vs. Levenshtein vs. Phonetic\n",
    "3. **Using `find_duplicates()`**: FuzzyRust's built-in clustering function\n",
    "4. **Handling edge cases**: Middle initials, suffixes (Jr., III), nicknames\n",
    "5. **Production strategies**: Blocking, thresholds, and human review workflows\n",
    "\n",
    "By the end, you'll have a pipeline that identifies duplicate groups automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. Install FuzzyRust: `pip install fuzzyrust`\n",
    "2. Download a name dataset:\n",
    "   - **Option A (Recommended)**: NC Voter Registration data\n",
    "     - Go to: https://www.ncsbe.gov/results-data/voter-registration-data\n",
    "     - Download a county file (any county, ~10-50K records)\n",
    "     - Extract and note the path to the `.txt` file\n",
    "   - **Option B**: Use the synthetic data provided below\n",
    "\n",
    "The NC Voter data is public record and contains realistic name variations‚Äîperfect for testing deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzyrust as fr\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"FuzzyRust loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nc_voter_data(filepath: str, limit: int = 1000) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Load names from NC Voter Registration data.\n",
    "    \n",
    "    The file is tab-separated with columns including:\n",
    "    last_name, first_name, middle_name, name_suffix_lbl\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        for i, row in enumerate(reader):\n",
    "            if i >= limit:\n",
    "                break\n",
    "            # Combine name parts\n",
    "            parts = [\n",
    "                row.get('first_name', '').strip(),\n",
    "                row.get('middle_name', '').strip(),\n",
    "                row.get('last_name', '').strip(),\n",
    "            ]\n",
    "            suffix = row.get('name_suffix_lbl', '').strip()\n",
    "            if suffix:\n",
    "                parts.append(suffix)\n",
    "            \n",
    "            full_name = ' '.join(p for p in parts if p)\n",
    "            if full_name:\n",
    "                records.append({\n",
    "                    'id': i,\n",
    "                    'name': full_name,\n",
    "                    'first': row.get('first_name', '').strip(),\n",
    "                    'last': row.get('last_name', '').strip(),\n",
    "                })\n",
    "    return records\n",
    "\n",
    "\n",
    "# Try to load NC Voter data\n",
    "DATA_PATH = Path(\"data/ncvoter_Statewide.txt\")  # Adjust path as needed\n",
    "\n",
    "if DATA_PATH.exists():\n",
    "    customers = load_nc_voter_data(str(DATA_PATH), limit=2000)\n",
    "    print(f\"Loaded {len(customers)} records from NC Voter data\")\n",
    "else:\n",
    "    print(f\"Dataset not found at {DATA_PATH}\")\n",
    "    print(\"Using synthetic CRM data with realistic name variations...\")\n",
    "    print()\n",
    "    \n",
    "    # Synthetic data with intentional duplicates\n",
    "    customers = [\n",
    "        # Group 1: John Smith variations\n",
    "        {\"id\": 1001, \"name\": \"John Smith\", \"first\": \"John\", \"last\": \"Smith\"},\n",
    "        {\"id\": 1042, \"name\": \"Jon Smith\", \"first\": \"Jon\", \"last\": \"Smith\"},\n",
    "        {\"id\": 1089, \"name\": \"JOHN SMITH\", \"first\": \"JOHN\", \"last\": \"SMITH\"},\n",
    "        {\"id\": 1156, \"name\": \"John A. Smith\", \"first\": \"John\", \"last\": \"Smith\"},\n",
    "        {\"id\": 1201, \"name\": \"John Smith Jr.\", \"first\": \"John\", \"last\": \"Smith\"},\n",
    "        {\"id\": 1245, \"name\": \"Johnny Smith\", \"first\": \"Johnny\", \"last\": \"Smith\"},\n",
    "        \n",
    "        # Group 2: Jane Doe variations\n",
    "        {\"id\": 2001, \"name\": \"Jane Doe\", \"first\": \"Jane\", \"last\": \"Doe\"},\n",
    "        {\"id\": 2055, \"name\": \"Jane M. Doe\", \"first\": \"Jane\", \"last\": \"Doe\"},\n",
    "        {\"id\": 2103, \"name\": \"Jayne Doe\", \"first\": \"Jayne\", \"last\": \"Doe\"},\n",
    "        {\"id\": 2178, \"name\": \"Jane Doe-Smith\", \"first\": \"Jane\", \"last\": \"Doe-Smith\"},\n",
    "        \n",
    "        # Group 3: Robert Johnson variations\n",
    "        {\"id\": 3001, \"name\": \"Robert Johnson\", \"first\": \"Robert\", \"last\": \"Johnson\"},\n",
    "        {\"id\": 3067, \"name\": \"Bob Johnson\", \"first\": \"Bob\", \"last\": \"Johnson\"},\n",
    "        {\"id\": 3124, \"name\": \"Rob Johnson\", \"first\": \"Rob\", \"last\": \"Johnson\"},\n",
    "        {\"id\": 3189, \"name\": \"Robert B. Johnson\", \"first\": \"Robert\", \"last\": \"Johnson\"},\n",
    "        {\"id\": 3256, \"name\": \"Robt Johnson\", \"first\": \"Robt\", \"last\": \"Johnson\"},\n",
    "        \n",
    "        # Group 4: William Davis variations  \n",
    "        {\"id\": 4001, \"name\": \"William Davis\", \"first\": \"William\", \"last\": \"Davis\"},\n",
    "        {\"id\": 4078, \"name\": \"Bill Davis\", \"first\": \"Bill\", \"last\": \"Davis\"},\n",
    "        {\"id\": 4134, \"name\": \"Will Davis\", \"first\": \"Will\", \"last\": \"Davis\"},\n",
    "        {\"id\": 4201, \"name\": \"Wm Davis\", \"first\": \"Wm\", \"last\": \"Davis\"},\n",
    "        {\"id\": 4267, \"name\": \"Billy Davis\", \"first\": \"Billy\", \"last\": \"Davis\"},\n",
    "        \n",
    "        # Group 5: Catherine Wilson variations\n",
    "        {\"id\": 5001, \"name\": \"Catherine Wilson\", \"first\": \"Catherine\", \"last\": \"Wilson\"},\n",
    "        {\"id\": 5089, \"name\": \"Katherine Wilson\", \"first\": \"Katherine\", \"last\": \"Wilson\"},\n",
    "        {\"id\": 5145, \"name\": \"Kate Wilson\", \"first\": \"Kate\", \"last\": \"Wilson\"},\n",
    "        {\"id\": 5212, \"name\": \"Cathy Wilson\", \"first\": \"Cathy\", \"last\": \"Wilson\"},\n",
    "        {\"id\": 5278, \"name\": \"Katie Wilson\", \"first\": \"Katie\", \"last\": \"Wilson\"},\n",
    "        \n",
    "        # Unique records (no duplicates)\n",
    "        {\"id\": 6001, \"name\": \"Michael Brown\", \"first\": \"Michael\", \"last\": \"Brown\"},\n",
    "        {\"id\": 6050, \"name\": \"Sarah Miller\", \"first\": \"Sarah\", \"last\": \"Miller\"},\n",
    "        {\"id\": 6100, \"name\": \"David Garcia\", \"first\": \"David\", \"last\": \"Garcia\"},\n",
    "        {\"id\": 6150, \"name\": \"Emily Martinez\", \"first\": \"Emily\", \"last\": \"Martinez\"},\n",
    "        {\"id\": 6200, \"name\": \"James Anderson\", \"first\": \"James\", \"last\": \"Anderson\"},\n",
    "        {\"id\": 6250, \"name\": \"Elizabeth Thomas\", \"first\": \"Elizabeth\", \"last\": \"Thomas\"},\n",
    "        {\"id\": 6300, \"name\": \"Christopher Lee\", \"first\": \"Christopher\", \"last\": \"Lee\"},\n",
    "        {\"id\": 6350, \"name\": \"Jennifer Taylor\", \"first\": \"Jennifer\", \"last\": \"Taylor\"},\n",
    "    ]\n",
    "    \n",
    "print(f\"\\nTotal records: {len(customers)}\")\n",
    "print(\"\\nSample records:\")\n",
    "for c in customers[:8]:\n",
    "    print(f\"  ID {c['id']}: {c['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Name Variations\n",
    "\n",
    "Before diving into algorithms, let's understand the types of variations we're dealing with:\n",
    "\n",
    "| Type | Example | Challenge |\n",
    "|------|---------|----------|\n",
    "| **Typos** | Jon ‚Üí John | Missing character |\n",
    "| **Case** | JOHN ‚Üí John | Uppercase entry |\n",
    "| **Middle names** | John Smith ‚Üí John A. Smith | Extra information |\n",
    "| **Suffixes** | John Smith ‚Üí John Smith Jr. | Generational suffix |\n",
    "| **Nicknames** | Robert ‚Üí Bob, William ‚Üí Bill | Completely different strings! |\n",
    "| **Phonetic** | Catherine ‚Üí Katherine | Same sound, different spelling |\n",
    "| **Abbreviations** | William ‚Üí Wm, Robert ‚Üí Robt | Historical conventions |\n",
    "\n",
    "The hardest cases are **nicknames** because they share no characters. \"Robert\" and \"Bob\" have zero overlap, yet they're the same person.\n",
    "\n",
    "Let's see how different algorithms handle these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different types of name variations\n",
    "test_pairs = [\n",
    "    (\"John Smith\", \"Jon Smith\", \"Typo\"),\n",
    "    (\"John Smith\", \"JOHN SMITH\", \"Case\"),\n",
    "    (\"John Smith\", \"John A. Smith\", \"Middle initial\"),\n",
    "    (\"John Smith\", \"John Smith Jr.\", \"Suffix\"),\n",
    "    (\"Robert Johnson\", \"Bob Johnson\", \"Nickname\"),\n",
    "    (\"Catherine Wilson\", \"Katherine Wilson\", \"Phonetic\"),\n",
    "    (\"William Davis\", \"Wm Davis\", \"Abbreviation\"),\n",
    "]\n",
    "\n",
    "print(\"How Different Algorithms Handle Name Variations\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Name Pair':<45} {'Type':<12} {'Lev':<8} {'Jaro-W':<8} {'Soundex'}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for name1, name2, variation_type in test_pairs:\n",
    "    # Case-insensitive comparisons\n",
    "    lev = fr.levenshtein_similarity_ci(name1, name2)\n",
    "    jw = fr.jaro_winkler_similarity_ci(name1, name2)\n",
    "    \n",
    "    # Phonetic - compare last names only (more meaningful)\n",
    "    last1 = name1.split()[-1]\n",
    "    last2 = name2.split()[-1]\n",
    "    phonetic = \"Match\" if fr.soundex_match(last1, last2) else \"No\"\n",
    "    \n",
    "    pair_display = f\"{name1} ‚Üî {name2}\"\n",
    "    print(f\"{pair_display:<45} {variation_type:<12} {lev:<8.3f} {jw:<8.3f} {phonetic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight: Jaro-Winkler for Names\n",
    "\n",
    "Notice that **Jaro-Winkler consistently scores higher** for name variations. This is because:\n",
    "\n",
    "1. **Prefix bonus**: Names often share the same beginning (\"John\" and \"Jon\" both start with \"Jo\")\n",
    "2. **Transposition tolerance**: Handles character swaps well\n",
    "3. **Length normalization**: Works well for varying-length names\n",
    "\n",
    "However, Jaro-Winkler still fails for nicknames (\"Robert\" vs \"Bob\"). For those, we need:\n",
    "- A nickname lookup table, OR\n",
    "- Phonetic matching on surnames + fuzzy matching on first names separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `find_duplicates()` for Automatic Clustering\n",
    "\n",
    "FuzzyRust's `find_duplicates()` function does the heavy lifting:\n",
    "\n",
    "1. Compares all pairs of strings above a similarity threshold\n",
    "2. Builds a graph where edges connect similar strings\n",
    "3. Finds connected components (clusters) in the graph\n",
    "4. Returns groups of duplicates\n",
    "\n",
    "This means if A matches B, and B matches C, all three end up in the same group‚Äîeven if A and C don't directly match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract just the names for deduplication\n",
    "names = [c[\"name\"] for c in customers]\n",
    "\n",
    "print(f\"Finding duplicates among {len(names)} names...\")\n",
    "print()\n",
    "\n",
    "# Run deduplication with Jaro-Winkler\n",
    "result = fr.find_duplicates(\n",
    "    names,\n",
    "    algorithm=\"jaro_winkler\",\n",
    "    min_similarity=0.85,      # 85% similarity threshold\n",
    "    normalize=\"lowercase\",    # Handle case variations\n",
    "    method=\"auto\"             # Automatically choose best method\n",
    ")\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"  Duplicate groups found: {len(result.groups)}\")\n",
    "print(f\"  Total duplicates: {result.total_duplicates}\")\n",
    "print(f\"  Unique records: {len(result.unique)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the duplicate groups with original IDs\n",
    "name_to_id = {c[\"name\"]: c[\"id\"] for c in customers}\n",
    "\n",
    "print(\"\\nDuplicate Groups:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, group in enumerate(result.groups, 1):\n",
    "    print(f\"\\nüìÅ Group {i} ({len(group)} records):\")\n",
    "    for name in group:\n",
    "        record_id = name_to_id.get(name, \"?\")\n",
    "        print(f\"   ID {record_id}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the Threshold\n",
    "\n",
    "The `min_similarity` threshold is critical:\n",
    "\n",
    "- **Too high (0.95+)**: Misses legitimate duplicates (\"Jon\" won't match \"John\")\n",
    "- **Too low (0.70-)**: Creates false positives (\"John Smith\" might match \"Jane Smith\")\n",
    "\n",
    "Let's see how different thresholds affect our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Effect of Threshold on Deduplication\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Threshold':<12} {'Groups':<10} {'Duplicates':<12} {'Unique'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for threshold in [0.95, 0.90, 0.85, 0.80, 0.75, 0.70]:\n",
    "    result = fr.find_duplicates(\n",
    "        names,\n",
    "        algorithm=\"jaro_winkler\",\n",
    "        min_similarity=threshold,\n",
    "        normalize=\"lowercase\"\n",
    "    )\n",
    "    print(f\"{threshold:<12.2f} {len(result.groups):<10} {result.total_duplicates:<12} {len(result.unique)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Nicknames: A Two-Stage Approach\n",
    "\n",
    "Nicknames are the hardest problem. \"Robert\" and \"Bob\" have essentially zero character overlap. \n",
    "\n",
    "There are two approaches:\n",
    "\n",
    "### Approach 1: Nickname Lookup Table\n",
    "Maintain a dictionary mapping nicknames to canonical names:\n",
    "```python\n",
    "nicknames = {\n",
    "    \"bob\": \"robert\", \"rob\": \"robert\", \"robbie\": \"robert\",\n",
    "    \"bill\": \"william\", \"will\": \"william\", \"wm\": \"william\",\n",
    "    \"kate\": \"catherine\", \"cathy\": \"catherine\", \"katie\": \"catherine\",\n",
    "}\n",
    "```\n",
    "\n",
    "### Approach 2: Two-Stage Matching\n",
    "1. **Stage 1**: Match on last name (must be similar)\n",
    "2. **Stage 2**: For matches, check if first names are similar OR known nickname pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common nickname mappings\n",
    "NICKNAMES = {\n",
    "    # Robert variants\n",
    "    \"bob\": \"robert\", \"rob\": \"robert\", \"robbie\": \"robert\", \"bobby\": \"robert\", \"robt\": \"robert\",\n",
    "    # William variants\n",
    "    \"bill\": \"william\", \"will\": \"william\", \"wm\": \"william\", \"billy\": \"william\", \"willie\": \"william\",\n",
    "    # Catherine/Katherine variants\n",
    "    \"kate\": \"catherine\", \"cathy\": \"catherine\", \"katie\": \"catherine\", \n",
    "    \"kathy\": \"katherine\", \"kat\": \"catherine\",\n",
    "    # John variants\n",
    "    \"jon\": \"john\", \"johnny\": \"john\", \"jack\": \"john\",\n",
    "    # James variants\n",
    "    \"jim\": \"james\", \"jimmy\": \"james\", \"jamie\": \"james\",\n",
    "    # Elizabeth variants\n",
    "    \"liz\": \"elizabeth\", \"beth\": \"elizabeth\", \"betty\": \"elizabeth\", \"lizzy\": \"elizabeth\",\n",
    "    # Michael variants\n",
    "    \"mike\": \"michael\", \"mikey\": \"michael\", \"mick\": \"michael\",\n",
    "    # Richard variants\n",
    "    \"rick\": \"richard\", \"dick\": \"richard\", \"rich\": \"richard\", \"ricky\": \"richard\",\n",
    "}\n",
    "\n",
    "def normalize_first_name(name: str) -> str:\n",
    "    \"\"\"Normalize first name using nickname table.\"\"\"\n",
    "    name_lower = name.lower().strip()\n",
    "    return NICKNAMES.get(name_lower, name_lower)\n",
    "\n",
    "def enhanced_name_match(name1: str, name2: str, threshold: float = 0.85) -> tuple[bool, float]:\n",
    "    \"\"\"\n",
    "    Check if two names match, considering nicknames.\n",
    "    \n",
    "    Returns: (is_match, confidence_score)\n",
    "    \"\"\"\n",
    "    # Split into first and last names\n",
    "    parts1 = name1.split()\n",
    "    parts2 = name2.split()\n",
    "    \n",
    "    if len(parts1) < 2 or len(parts2) < 2:\n",
    "        # Fall back to full name comparison\n",
    "        score = fr.jaro_winkler_similarity_ci(name1, name2)\n",
    "        return score >= threshold, score\n",
    "    \n",
    "    first1, last1 = parts1[0], parts1[-1]\n",
    "    first2, last2 = parts2[0], parts2[-1]\n",
    "    \n",
    "    # Last names must match well\n",
    "    last_score = fr.jaro_winkler_similarity_ci(last1, last2)\n",
    "    if last_score < 0.85:\n",
    "        return False, last_score\n",
    "    \n",
    "    # Check first names\n",
    "    first_score = fr.jaro_winkler_similarity_ci(first1, first2)\n",
    "    \n",
    "    # Check nickname equivalence\n",
    "    norm1 = normalize_first_name(first1)\n",
    "    norm2 = normalize_first_name(first2)\n",
    "    nickname_match = (norm1 == norm2)\n",
    "    \n",
    "    # Calculate combined score\n",
    "    if nickname_match:\n",
    "        combined_score = (0.5 + last_score) / 1.5  # Boost for nickname match\n",
    "        return True, combined_score\n",
    "    elif first_score >= threshold:\n",
    "        combined_score = (first_score + last_score) / 2\n",
    "        return combined_score >= threshold, combined_score\n",
    "    \n",
    "    return False, (first_score + last_score) / 2\n",
    "\n",
    "# Test the enhanced matching\n",
    "print(\"Enhanced Name Matching with Nickname Support\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_pairs = [\n",
    "    (\"Robert Johnson\", \"Bob Johnson\"),\n",
    "    (\"William Davis\", \"Bill Davis\"),\n",
    "    (\"Catherine Wilson\", \"Kate Wilson\"),\n",
    "    (\"John Smith\", \"Jon Smith\"),\n",
    "    (\"John Smith\", \"Jane Smith\"),  # Different person\n",
    "]\n",
    "\n",
    "for name1, name2 in test_pairs:\n",
    "    match, score = enhanced_name_match(name1, name2)\n",
    "    status = \"‚úì Match\" if match else \"‚úó No match\"\n",
    "    print(f\"{name1:<20} ‚Üî {name2:<20} ‚Üí {status} ({score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonetic Matching for Similar-Sounding Names\n",
    "\n",
    "Some names sound the same but are spelled differently:\n",
    "- Catherine / Katherine\n",
    "- Steven / Stephen  \n",
    "- Smith / Smyth\n",
    "\n",
    "Phonetic algorithms encode names by how they sound. FuzzyRust provides:\n",
    "- **Soundex**: Classic algorithm, 4-character codes\n",
    "- **Metaphone**: More accurate, variable-length codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phonetic matching examples\n",
    "phonetic_pairs = [\n",
    "    (\"Catherine\", \"Katherine\"),\n",
    "    (\"Steven\", \"Stephen\"),\n",
    "    (\"Smith\", \"Smyth\"),\n",
    "    (\"Meyer\", \"Mayer\"),\n",
    "    (\"Thompson\", \"Thomson\"),\n",
    "    (\"Schneider\", \"Snyder\"),\n",
    "    (\"John\", \"Jane\"),  # Different - should NOT match\n",
    "]\n",
    "\n",
    "print(\"Phonetic Matching: Soundex vs Metaphone\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Name 1':<15} {'Name 2':<15} {'Soundex 1':<10} {'Soundex 2':<10} {'Match?'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for n1, n2 in phonetic_pairs:\n",
    "    s1 = fr.soundex(n1)\n",
    "    s2 = fr.soundex(n2)\n",
    "    match = \"‚úì\" if s1 == s2 else \"‚úó\"\n",
    "    print(f\"{n1:<15} {n2:<15} {s1:<10} {s2:<10} {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Pipeline: Blocking + Detailed Comparison\n",
    "\n",
    "For large datasets (100K+ records), comparing every pair is too slow. \n",
    "\n",
    "**Blocking** reduces the search space by only comparing records that share a common attribute:\n",
    "\n",
    "1. **First letter of last name**: \"Smith\" only compared to other \"S\" names\n",
    "2. **Soundex code**: Group phonetically similar names\n",
    "3. **First 3 characters**: Quick pre-filter\n",
    "\n",
    "This reduces O(n¬≤) comparisons to O(n √ó block_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blocks(records: list[dict], method: str = \"soundex_last\") -> dict:\n",
    "    \"\"\"\n",
    "    Group records into blocks for efficient comparison.\n",
    "    \n",
    "    Args:\n",
    "        records: List of customer records with 'name' and 'last' fields\n",
    "        method: Blocking method - 'soundex_last', 'first_letter', or 'first_3'\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping block keys to lists of records\n",
    "    \"\"\"\n",
    "    blocks = defaultdict(list)\n",
    "    \n",
    "    for record in records:\n",
    "        last_name = record.get('last', record['name'].split()[-1])\n",
    "        \n",
    "        if method == \"soundex_last\":\n",
    "            key = fr.soundex(last_name)\n",
    "        elif method == \"first_letter\":\n",
    "            key = last_name[0].upper() if last_name else \"?\"\n",
    "        elif method == \"first_3\":\n",
    "            key = last_name[:3].lower() if len(last_name) >= 3 else last_name.lower()\n",
    "        else:\n",
    "            key = \"all\"  # No blocking\n",
    "        \n",
    "        blocks[key].append(record)\n",
    "    \n",
    "    return dict(blocks)\n",
    "\n",
    "# Create blocks\n",
    "blocks = create_blocks(customers, method=\"soundex_last\")\n",
    "\n",
    "print(f\"Blocking Statistics (Soundex on Last Name)\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Total records: {len(customers)}\")\n",
    "print(f\"Number of blocks: {len(blocks)}\")\n",
    "print(f\"\")\n",
    "\n",
    "# Show block sizes\n",
    "print(\"Largest blocks:\")\n",
    "sorted_blocks = sorted(blocks.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "for key, records in sorted_blocks[:5]:\n",
    "    sample_names = [r['last'] for r in records[:3]]\n",
    "    print(f\"  Block '{key}': {len(records)} records (e.g., {', '.join(sample_names)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates_with_blocking(\n",
    "    records: list[dict],\n",
    "    blocking_method: str = \"soundex_last\",\n",
    "    threshold: float = 0.85\n",
    ") -> list[list[dict]]:\n",
    "    \"\"\"\n",
    "    Find duplicates using blocking + detailed comparison.\n",
    "    \n",
    "    Returns list of duplicate groups (each group is a list of records).\n",
    "    \"\"\"\n",
    "    blocks = create_blocks(records, blocking_method)\n",
    "    all_groups = []\n",
    "    \n",
    "    for block_key, block_records in blocks.items():\n",
    "        if len(block_records) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Run deduplication within this block\n",
    "        names = [r[\"name\"] for r in block_records]\n",
    "        result = fr.find_duplicates(\n",
    "            names,\n",
    "            algorithm=\"jaro_winkler\",\n",
    "            min_similarity=threshold,\n",
    "            normalize=\"lowercase\"\n",
    "        )\n",
    "        \n",
    "        # Map names back to full records\n",
    "        name_to_record = {r[\"name\"]: r for r in block_records}\n",
    "        for group in result.groups:\n",
    "            record_group = [name_to_record[name] for name in group]\n",
    "            all_groups.append(record_group)\n",
    "    \n",
    "    return all_groups\n",
    "\n",
    "# Run blocked deduplication\n",
    "duplicate_groups = find_duplicates_with_blocking(\n",
    "    customers,\n",
    "    blocking_method=\"soundex_last\",\n",
    "    threshold=0.85\n",
    ")\n",
    "\n",
    "print(f\"\\nBlocked Deduplication Results\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Duplicate groups found: {len(duplicate_groups)}\")\n",
    "\n",
    "for i, group in enumerate(duplicate_groups, 1):\n",
    "    print(f\"\\nGroup {i}:\")\n",
    "    for record in group:\n",
    "        print(f\"  ID {record['id']}: {record['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Review Workflow\n",
    "\n",
    "In production, automated deduplication should flag candidates for human review rather than auto-merging. Here's a simple workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_review_report(duplicate_groups: list, min_confidence: float = 0.9) -> dict:\n",
    "    \"\"\"\n",
    "    Generate a human review report for duplicate candidates.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with 'auto_merge' (high confidence) and 'review' (needs human eyes)\n",
    "    \"\"\"\n",
    "    auto_merge = []\n",
    "    needs_review = []\n",
    "    \n",
    "    for group in duplicate_groups:\n",
    "        if len(group) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Calculate pairwise similarity for the group\n",
    "        names = [r[\"name\"] for r in group]\n",
    "        similarities = []\n",
    "        for i, n1 in enumerate(names):\n",
    "            for n2 in names[i+1:]:\n",
    "                sim = fr.jaro_winkler_similarity_ci(n1, n2)\n",
    "                similarities.append(sim)\n",
    "        \n",
    "        avg_sim = sum(similarities) / len(similarities) if similarities else 0\n",
    "        min_sim = min(similarities) if similarities else 0\n",
    "        \n",
    "        group_info = {\n",
    "            \"records\": group,\n",
    "            \"avg_similarity\": avg_sim,\n",
    "            \"min_similarity\": min_sim,\n",
    "        }\n",
    "        \n",
    "        # High confidence if ALL pairs are above threshold\n",
    "        if min_sim >= min_confidence:\n",
    "            auto_merge.append(group_info)\n",
    "        else:\n",
    "            needs_review.append(group_info)\n",
    "    \n",
    "    return {\"auto_merge\": auto_merge, \"review\": needs_review}\n",
    "\n",
    "# Generate report\n",
    "report = generate_review_report(duplicate_groups, min_confidence=0.92)\n",
    "\n",
    "print(\"Deduplication Review Report\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n‚úÖ AUTO-MERGE ({len(report['auto_merge'])} groups):\")\n",
    "print(\"   High confidence - can be merged automatically\")\n",
    "\n",
    "for group_info in report['auto_merge']:\n",
    "    print(f\"\\n   Group (avg: {group_info['avg_similarity']:.1%}):\")\n",
    "    for r in group_info['records']:\n",
    "        print(f\"     - {r['name']} (ID: {r['id']})\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  NEEDS REVIEW ({len(report['review'])} groups):\")\n",
    "print(\"   Lower confidence - human verification recommended\")\n",
    "\n",
    "for group_info in report['review']:\n",
    "    print(f\"\\n   Group (avg: {group_info['avg_similarity']:.1%}, min: {group_info['min_similarity']:.1%}):\")\n",
    "    for r in group_info['records']:\n",
    "        print(f\"     - {r['name']} (ID: {r['id']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Use Jaro-Winkler for names**: Its prefix bonus works well with first/last name patterns.\n",
    "\n",
    "2. **Normalize aggressively**: Always use case-insensitive matching; consider removing punctuation.\n",
    "\n",
    "3. **Handle nicknames explicitly**: No algorithm will match \"Robert\" to \"Bob\"‚Äîuse a lookup table.\n",
    "\n",
    "4. **Use blocking for scale**: Don't compare every pair; group by last name soundex or similar.\n",
    "\n",
    "5. **Implement human review**: Auto-merge high-confidence matches; flag others for review.\n",
    "\n",
    "6. **Tune thresholds carefully**:\n",
    "   - 0.90+: Conservative, few false positives\n",
    "   - 0.85: Good balance\n",
    "   - 0.80-: Aggressive, may need more review\n",
    "\n",
    "### When to Use This Approach\n",
    "\n",
    "‚úÖ **Good for:**\n",
    "- CRM deduplication\n",
    "- Patient record matching\n",
    "- Customer data cleaning\n",
    "- Voter roll maintenance\n",
    "\n",
    "‚ùå **Consider alternatives for:**\n",
    "- Multi-attribute matching (add address, DOB) ‚Üí use `SchemaIndex`\n",
    "- Very large datasets (10M+) ‚Üí consider probabilistic matching\n",
    "- Real-time matching ‚Üí pre-compute blocks, use indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference: Complete deduplication pipeline\n",
    "print(\"Quick Reference: Complete Deduplication Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "1. LOAD DATA\n",
    "   records = load_customer_data(\"customers.csv\")\n",
    "\n",
    "2. EXTRACT NAMES\n",
    "   names = [r[\"name\"] for r in records]\n",
    "\n",
    "3. FIND DUPLICATES\n",
    "   result = fr.find_duplicates(\n",
    "       names,\n",
    "       algorithm=\"jaro_winkler\",\n",
    "       min_similarity=0.85,\n",
    "       normalize=\"lowercase\"\n",
    "   )\n",
    "\n",
    "4. REVIEW RESULTS\n",
    "   for group in result.groups:\n",
    "       print(f\"Potential duplicates: {group}\")\n",
    "\n",
    "5. (OPTIONAL) BLOCKING FOR SCALE\n",
    "   blocks = create_blocks(records, \"soundex_last\")\n",
    "   for block in blocks.values():\n",
    "       # Deduplicate within each block\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
